# Ref: https://grafana.com/docs/loki/latest/configure/

auth_enabled: false

limits_config:
  max_label_names_per_series: 30
  ingestion_rate_mb: 256
  ingestion_burst_size_mb: 256
  deletion_mode: filter-and-delete
  retention_period: 168h
  retention_stream:
  - selector: '{namespace="namespace-to-set-retention"}'
    priority: 2
    period: 17568h
  - selector: '{namespace=~"namespace-to-set-retention"}'
    priority: 1
    period: 744h

common:
  ring:
    kvstore:
      store: inmemory

compactor:
  working_directory: /data/loki/compactor
  retention_enabled: true
  delete_request_store: filesystem

ingester:
  lifecycler:
    ring:
      replication_factor: 1
  wal:
    dir: /data/loki/wal

ruler:
  storage:
    type: local
    local:
      directory: /tmp/rules
  rule_path: /tmp/scratch
  alertmanager_url: http://prometheus-kube-prometheus-alertmanager:9093
  ring:
    kvstore:
      store: inmemory
  enable_api: true

server:
  log_level: warn

schema_config:
  configs:
  - from: 2025-01-01
    store: tsdb
    object_store: filesystem
    schema: v13
    index:
      prefix: index_
      period: 24h

storage_config:
  filesystem:
    directory: /data/loki/chunks
  tsdb_shipper:
    active_index_directory: /data/tsdb-index
    cache_location: /data/tsdb-cache

query_scheduler:
  # the TSDB index dispatches many more, but each individually smaller, requests. 
  # We increase the pending request queue sizes to compensate.
  max_outstanding_requests_per_tenant: 32768

querier:
  # Each `querier` component process runs a number of parallel workers to process queries simultaneously.
  # You may want to adjust this up or down depending on your resource usage
  # (more available cpu and memory can tolerate higher values and vice versa),
  # but we find the most success running at around `16` with tsdb
  max_concurrent: 16
